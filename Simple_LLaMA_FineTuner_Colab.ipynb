{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFPLo0XJoxb8"
      },
      "source": [
        "# ðŸ¦™ Simple LLaMA Finetuner\n",
        "Many thanks to LXE: https://github.com/lxe/simple-llama-finetuner\n",
        "\n",
        "The v0 of this notebook allows you to..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8goOFKNao5db"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU (5 seconds, run once per session)\n",
        "#@markdown Check type of GPU and VRAM available. A minimum of 15000 MiB VRAM is required. If no GPU, go to the Colab menu Runtime > Change Runtime Type and select GPU\n",
        "\n",
        "# !if nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader | grep -q 'MiB'; then \\\n",
        "#   echo \"You have a gpu and can proceed! Your GPU is:\" ; \\\n",
        "# fi\n",
        "# !nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader\n",
        "\n",
        "from subprocess import getoutput\n",
        "import os\n",
        "\n",
        "gpu_info = getoutput('nvidia-smi  --query-gpu=name --format=csv,noheader')\n",
        "if(\"A100\" in gpu_info):\n",
        "    which_gpu = \"A10G\"\n",
        "    # os.system(f\"pip install -q https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl\")\n",
        "elif(\"T4\" in gpu_info):\n",
        "    which_gpu = \"T4\"\n",
        "    # os.system(f\"pip install --force-reinstall -q https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+1515f77.d20221130-cp38-cp38-linux_x86_64.whl\")\n",
        "elif(\"V100\" in gpu_info):\n",
        "    which_gpu = \"V100\"\n",
        "elif \"failed\" in gpu_info:\n",
        "    print(gpu_info)\n",
        "    which_gpu = \"CPU\"\n",
        "else:\n",
        "    which_gpu = gpu_info\n",
        "\n",
        "if which_gpu != \"CPU\":\n",
        "    free_vram = getoutput('nvidia-smi  --query-gpu=memory.free --format=csv,noheader')\n",
        "    print(f\"Congrats! You've got a GPU! It's a {which_gpu} model gpu with {free_vram} VRAM\")\n",
        "else:\n",
        "    print(\"Boo, no gpu available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPj87UM9pHRH"
      },
      "outputs": [],
      "source": [
        "#@title Allow Google Drive & set paths (2 minutes, run once per session)\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# local_path = '/content/env'\n",
        "# env_path = '/content/drive/MyDrive/colab_envs/asdf'\n",
        "# os.makedirs(env_path, exist_ok=True)\n",
        "# !ln -s $env_path $local_path\n",
        "# sys.path.insert(0,env_path)\n",
        "\n",
        "save_to_gdrive = True\n",
        "\n",
        "#@markdown Huggingface's name/path of the initial model, this is a path in huggingface's repository (you probably don't want to change this)\n",
        "MODEL_PATH = \"decapoda-research/llama-13b-hf\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save your newly-trained model weights. If using gdrive, this will store in your gdrive root folder.\n",
        "OUTPUT_DIR = \"simple-llama-finetuner\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user on ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "# https://huggingface.co/settings/tokens\n",
        "!mkdir -p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"\" #@param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token\n",
        "\n",
        "print(\"Done!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lp4UHZKyBmK"
      },
      "outputs": [],
      "source": [
        "#@title Model/dependency setup (2 minutes, run once per session)\n",
        "\n",
        "import os.path\n",
        "\n",
        "if not os.path.isfile(OUTPUT_DIR+\"/main.py\"):\n",
        "  !git clone https://github.com/jimmoffet/simple-llama-finetuner.git $OUTPUT_DIR\n",
        "else:\n",
        "  print(\"Working directory looks good.\")\n",
        "\n",
        "try:\n",
        "  import peft\n",
        "  print(\"Dependencies look good.\")\n",
        "except:\n",
        "  !cd $OUTPUT_DIR && git pull && pip install -r requirements.txt\n",
        "  print(\"Dependencies look good.\")\n",
        "\n",
        "print(\"Done!\")\n",
        "# !git clone https://github.com/lxe/simple-llama-finetuner.git $OUTPUT_DIR\n",
        "# !cd $OUTPUT_DIR && git pull && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrpQlYH-0HII"
      },
      "outputs": [],
      "source": [
        "#@title Run gradio UI (1 minute, run once per session)\n",
        "!cd $OUTPUT_DIR && python main.py --share --path $MODEL_PATH"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
